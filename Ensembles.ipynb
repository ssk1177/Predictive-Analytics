{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ensembles","metadata":{"tags":[]}},{"cell_type":"markdown","source":"# Table of contents <a class=\"anchor\" id=\"toc\"></a>\n* [Load the Packages](#load_packages)\n* [Functional Blocks](#functional_blocks)\n* [Load the Dataset](#load_dataset)\n* [Explore the dataset](#explore_dataset)\n* [Preprocessing](#preprocessing)\n    * [Missing values](#missing_values)\n    * [Drop the irrelevant features](#drop_irrelevant)\n    * [Replace missing values](#replace_missing)\n    * [Feature Encoding](#feature_encoding)\n* [Feature Selection](#feature_selection)\n* [Feature Engineering](#feature_engineering)\n    * [Binning](#binning)\n* [Pipeline](#pipeline)\n* [Data split: Train and Test sets](#data_split)\n* [Modelling](#modelling)\n    * [Decision Tree](#decision_tree)\n        * [Classifier Pipeline](#dt_classifier_pipeline)\n        * [Feature Importance](#feature_importance)\n        * [Metrics](#dt_metrics)\n            * [Classification Report](#dt_classification_report)\n            * [Confusion Matrix](#dt_confusion_matrix)\n        * [Tree Structure](#tree_structure)\n        * [Tree Pruning](#tree_pruning)\n            * [Retrain with Tree Pruning](#retrain_with_pruning)\n            * [Metrics](#rp_metrics)\n                * [Classification Report](#rp_classification_report)\n                * [Confusion Matrix](#rp_confusion_matrix)\n                * [Feature Importance](#rp_feature_imp)\n        * [Hyper-parameters Tuning](#hyper_params_tuning)\n            * [Retrain with Hyper-parameters Tuned](#hp_tuned_retrain)\n            * [Metrics](#hp_tuned_metrics)\n                * [Comfusion Matrix](#hp_confusion_matrix)\n    * [Random Forest](#random_forest)\n        * [Metrics](#rf_metrics)\n            * [Classification Report](#rf_classification_report)\n            * [Confusion Matrix](#rf_confusion_matrix)\n        * [Feature Importance](#rf_feature_imp)\n    * [Gradient Boosting](#gradiet_boosting)\n        * [Metrics](#gb_metrics)\n            * [Classification Report](#gb_classification_report)\n            * [Confusion Matrix](#gb_confusion_matrix)\n        * [Feature Importance](#gb_feature_importance)\n    * [AdaBoost](#AdaBoost)\n        * [Metrics](#AB_metrics)\n            * [Classification Report](#AB_classification_report)\n            * [Confusion Matrix](#AB_confusion_matrix)\n        * [Feature Importance](#AB_feature_importance)\n* [Models Comparison](#models_comparison)","metadata":{}},{"cell_type":"markdown","source":"## Load the Packages <a class=\"anchor\" id=\"load_packages\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport time\n\n# sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error\nfrom sklearn.tree import plot_tree, DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn import tree, set_config, metrics\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n\nfrom IPython.display import HTML, display, Markdown as md, Latex\nfrom ipywidgets import widgets, Layout\nfrom ipywidgets import Output, Tab\nfrom matplotlib import pyplot as plt\n\n# check scikit-learn version\nfrom sklearn import __version__ as ver\nprint(f\"scikit-learn version: {ver}\")\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functional Blocks <a class=\"anchor\" id=\"functional_blocks\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"def feat_imp_chart(feat_imp, feat_names, plot_head):\n    importances = pd.Series(feat_imp, index=feat_names).sort_values(ascending=True)\n    importances.plot.barh()\n    plt.title(plot_head)\n    plt.show()\n\ndef get_top_5(data):\n    return np.sort(data)[::-1][0:5]","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the Titanic Dataset (Source: openml.org) <a class=\"anchor\" id=\"load_dataset\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explore the dataset <a class=\"anchor\" id=\"explore_dataset\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"X.info()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations\n- Features 'cabin', 'boat', 'body', and 'home.dest' has very few non-null values.\n- Feature 'sex' and 'embarked' are of type category and would require encoding (?)","metadata":{"tags":[]}},{"cell_type":"code","source":"X.describe()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations\n- Features such as 'age', 'fare' and 'body' are of different ranges than 'pclass' and 'sibsp' and 'parch', might require scaling.","metadata":{"tags":[]}},{"cell_type":"code","source":"y.info()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.pie (y.value_counts(), labels=['not-survived', 'survived'], autopct='%1.0f%%')\nplt.show()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing <a class=\"anchor\" id=\"preprocessing\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"### Missing values <a class=\"anchor\" id=\"missing_values\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"# Check for missing values\nX.isnull().any()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check how much percentage of missing values in feature\nX.isnull().sum() / len(X) * 100","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the data types of features\nX.dtypes","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations\n1. Features 'body', 'boat' and 'cabin' has more than 60% missing values, so we can drop these features (though the more research is needed to verify whether it's ideal to remove or fill the concerned features, but for simplicity sake for this lab, I am removing it)\n2. Features 'age', 'fare' and 'embarked' has the null/missing values, but features 'age' and 'fare' are of numerical type so will replace the values using median while 'embarked' is categorical type, hence it makes more sense to replace it with the mode value.\n3. Replacing the feature 'home.dest' (which represents the address) doesn't make much sense, so for simplicity of the model, removing this feature.\n4. Feature 'ticket' is not relevant to the objective of our report, so removing it.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"### Drop the irrelevant features <a class=\"anchor\" id=\"drop_irrelevant\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"# On basis of the above observations, dropping following irrelevant features\nX.drop([\"cabin\", \"boat\", \"body\", \"home.dest\", \"ticket\"], axis=1, inplace=True)\nX.head()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Replace missing values <a class=\"anchor\" id=\"replace_missing\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"# Replacing missing values\nX[\"age\"].fillna(X[\"age\"].median(), inplace=True)\nX[\"fare\"].fillna(X[\"fare\"].median(), inplace=True)\nX[\"embarked\"].fillna(X[\"embarked\"].mode()[0], inplace=True)\n\n# Verify whether any missing value in any feature\nX.isnull().any()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Encoding <a class=\"anchor\" id=\"feature_encoding\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>\nConvert categorical features to numerical","metadata":{"tags":[]}},{"cell_type":"code","source":"le = LabelEncoder()\nX[\"sex\"] = le.fit_transform(X[\"sex\"])\nX[\"embarked\"] = le.fit_transform(X[\"embarked\"])\n\n# Verify the dataset\nX.head()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection <a class=\"anchor\" id=\"feature_selection\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>\nExamine Feature Correlations","metadata":{"tags":[]}},{"cell_type":"code","source":"# Since for correlation we need only numerical values, so \nsns.heatmap(X[[\"pclass\", \"sex\", \"age\", \"sibsp\", \"parch\", \"fare\", \"embarked\"]].corr(), annot=True, cmap='coolwarm')\nplt.title(\"Correlation Matrix\")\nplt.show()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:\nFeatures 'parch' and 'sibsp' are weekly correlated featues, so we can combine them to get the more meaningful information, which lead to feature engineering.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"## Feature Engineering <a class=\"anchor\" id=\"feature_engineering\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"# Combining 'parch' and 'sibsp' to create new feature named as 'family_size' \n\nX['family_size'] = X['parch'] + X['sibsp']\nX.drop(['parch', 'sibsp'], axis=1, inplace=True)\n\n# Create a derived feature called 'is_alone' using the family_size feature\nX['is_alone'] = 1\nX['is_alone'].loc[X['family_size'] > 1] = 0\n\n# Print the head to verify the data\nX.head()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using 'name' feature we can derive a new fearture called 'Title' which \n# will have values such as 'Mr.' and 'Mrs', 'Cpt.' and 'Dr.', \n# these values seems be of interest for our model\n\n# Create new feature\nX['title'] =  X['name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\n# Remove the 'name' feature\nX.drop([\"name\"], axis=1, inplace=True)\n\n# Print the head to verify the data\nX.head()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verify the new feature 'title'\npd.crosstab(X['title'], X['sex'])","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation\nIt seems like there are so many titles, so we can perform binning or grouping.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"### Binning <a class=\"anchor\" id=\"binning\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"# Mark the 'title' as 'rare' if the value is less than 10\nrare_titles = (X['title'].value_counts() < 10)\nrare_titles\n\nX.title.loc[X.title == 'Miss'] = 'Mrs'\nX['title'] = X.title.apply(lambda x: 'rare' if rare_titles[x] else x)\n\n# Verify the new feature 'title'\npd.crosstab(X['title'], X['sex'])","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pipeline <a class=\"anchor\" id=\"pipeline\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>\nCreate preprocessing (numerical, categorical transform) pipeline","metadata":{"tags":[]}},{"cell_type":"code","source":"# here we call the new API set_config to tell sklearn we want to output a pandas DF\nset_config(transform_output=\"pandas\")\n\nnum_features = ['age', 'fare', 'family_size']\ncat_features = ['embarked', 'sex', 'pclass', 'title', 'is_alone']\n\n# creating the numerical pipeline\nnum_pipe = Pipeline([\n    ('imputer', SimpleImputer()),\n    ('scaler', StandardScaler())\n])\n\n#creating the transform to preprocess the data\ntransformer = ColumnTransformer(\n    (\n        ('numerical', num_pipe, num_features),\n        (\"categorical\", \n             OneHotEncoder(sparse_output=False, \n                           drop=\"if_binary\", \n                           handle_unknown=\"ignore\"), \n             cat_features\n        )\n    ),\n    verbose_feature_names_out=False,\n)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data split: Train and Test sets <a class=\"anchor\" id=\"data_split\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"# split the data for training and testing the model\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=117)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling <a class=\"anchor\" id=\"modelling\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>\nFollowing Algorithms would be trained and compared:\n- Decision Tree Classifier\n- Random Forest, \n- Gradient Boosted and \n- Adaboost ","metadata":{"tags":[]}},{"cell_type":"markdown","source":"### Decision Tree <a class=\"anchor\" id=\"decision_tree\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"### Classifier Pipeline <a class=\"anchor\" id=\"dt_classifier_pipeline\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>\nCreate classifer pipeline with a data preprocessing step and decision tree classifier","metadata":{"tags":[]}},{"cell_type":"code","source":"# creating the classifier pipeline with a data preprocessing step and decision tree classifier\nrf_pipeline = Pipeline([\n    ('dataprep', transformer),\n    ('rf_clf', DecisionTreeClassifier(random_state=117))\n])","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training the model\ndt_train_start_time = time.time()\nrf_pipeline.fit(X_train, y_train)\ndt_train_end_time = time.time()\n\ndt_train_time = round(dt_train_end_time - dt_train_start_time, 4)\nprint(\"Decision Tree Train time:\", dt_train_time, \"secs\\n\\n\")","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance <a class=\"anchor\" id=\"feature_importance\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"# retrieving the RF Classifier from the model pipeline\nclf = rf_pipeline[-1]\n\nprint(clf.feature_names_in_)\nclf.feature_names_in_[clf.feature_names_in_ == 'sex_1'] = 'sex'\n\n# making a pandas dataframe\ndata = list(zip(clf.feature_names_in_, clf.feature_importances_))\ndf_importances = pd.DataFrame(data, \n                              columns=['Feature', 'Importance']).sort_values(by='Importance', ascending=False)\n\ndf_importances","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot feature importance chart\nfeat_imp_chart(get_top_5(clf.feature_importances_), get_top_5(clf.feature_names_in_), 'Top 5 Features by Importance')\nclf_before_prune = clf","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metrics <a class=\"anchor\" id=\"dt_metrics\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"X_preproc = rf_pipeline[:-1].transform(X_train)\nX_test_preproc = rf_pipeline[:-1].transform(X_test)\n\nX_preproc.rename({'sex_1':'sex'}, axis=1, inplace=True)\nX_test_preproc.rename({'sex_1':'sex'}, axis=1, inplace=True)\n\ndt_train_acc = clf.score(X_preproc, y_train)\ndt_test_acc = clf.score(X_test_preproc, y_test)\nprint(f\"Train accuracy: {dt_train_acc:.3f}\")\nprint(f\"Test accuracy: {dt_test_acc:.3f}\")","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation\nSince the Train accuracy is much higher than the Test accuracy, it seems the model overfits the titanic dataset, we can further verify this using cross-validation.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"#### Classification Report <a class=\"anchor\" id=\"dt_classification_report\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"cf_test_start_time = time.time()\nyhat = clf.predict(X_test_preproc)\ncf_test_end_time = time.time()\n\ndt_test_time = round(cf_test_end_time - cf_test_start_time, 4)\n\nprint(\"Test time:\", dt_test_time, \"secs\\n\\n\")\n\nprint(classification_report(y_test, yhat))","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Confusion Matrix <a class=\"anchor\" id=\"dt_confusion_matrix\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, yhat)\ncm_display = ConfusionMatrixDisplay(cm).plot()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Tree Structure <a class=\"anchor\" id=\"tree_structure\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"plot_tree(clf)\nplt.show()\nprint(f\"Decision Tree Depth: {clf.get_depth()}\")\nprint(f\"Decision Tree Node Count: {clf.tree_.node_count}\")","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tree Pruning <a class=\"anchor\" id=\"tree_pruning\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"path = clf.cost_complexity_pruning_path(X_preproc, \n                                        y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n# Wee train a decision tree using the effective alphas. The last value\n# in ``ccp_alphas`` is the alpha value that prunes the whole tree,\n# leaving the tree, ``clfs[-1]``, with one node.\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=117, ccp_alpha=ccp_alpha)\n    clf.fit(X_preproc, \n            y_train)\n    clfs.append(clf)\nprint(\n    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n        clfs[-1].tree_.node_count, ccp_alphas[-1]\n    )\n)\n\n# We remove the last element in ``clfs`` and ``ccp_alphas``, \n# because it is the trivial tree with only one node. \nclfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n\n# Accuracy vs alpha for training and testing sets\ntrain_scores = [clf.score(X_preproc, \n                          y_train) for clf in clfs]\ntest_scores = [clf.score(X_test_preproc, \n                         y_test) for clf in clfs]\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='.', label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='.', label=\"test\", drawstyle=\"steps-post\")\nax.legend()\nax.grid()\nplt.tight_layout()\nplt.show()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Retrain with Pruning <a class=\"anchor\" id=\"retrain_with_pruning\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"clf_after_prune = DecisionTreeClassifier(random_state=117, ccp_alpha=0.01)\n\ncfp_train_start_time = time.time()\nclf_after_prune.fit(X_preproc, y_train)\ncfp_train_end_time = time.time()\n\ndtp_train_time = round(cfp_train_end_time-cfp_train_start_time, 4)\n\nprint(\"Training time:\", dtp_train_time, \"secs\")\n\ndtp_train_acc = clf_after_prune.score(X_preproc, y_train)\ndtp_test_acc = clf_after_prune.score(X_test_preproc, y_test)\n\nprint(f\"Train accuracy: {dtp_train_acc:.3f}\")\nprint(f\"Test accuracy: {dtp_test_acc:.3f}\")","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metrics <a class=\"anchor\" id=\"rp_metrics\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"print(f\"Decision Tree Depth: {clf_after_prune.get_depth()}\")\nprint(f\"Decision Tree Node Count: {clf_after_prune.tree_.node_count}\")","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_tree(clf_after_prune, feature_names=list(clf_after_prune.feature_names_in_), filled=True)\nplt.show()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Classification Report <a class=\"anchor\" id=\"rp_classification_report\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"cfp_test_start_time = time.time()\nyhat_after_prune = clf.predict(X_test_preproc)\ncfp_test_end_time = time.time()\n\ndtp_test_time = round(cfp_test_end_time-cfp_test_start_time, 4)\nprint(\"Testing time:\", dtp_test_time, \"secs\\n\\n\")\n\nprint(classification_report(y_test, yhat_after_prune))","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Confusion Matrix <a class=\"anchor\" id=\"rp_confusion_matrix\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, yhat_after_prune)\ncm_display = ConfusionMatrixDisplay(cm).plot()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Importance <a class=\"anchor\" id=\"rp_feature_imp\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"# Plot feature importance chart\nfeat_imp_chart(get_top_5(clf_after_prune.feature_importances_), \n               get_top_5(clf_after_prune.feature_names_in_),\n               'Top 5 Features by Importance (Post-Pruning)')","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyper-parameters tuning <a class=\"anchor\" id=\"hyper_params_tuning\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"# List of hyper-parameters supported by decision tree\n# 'ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', \n# 'min_impurity_decrease', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', \n# 'random_state', 'splitter'\n\ndtree_reg = DecisionTreeRegressor(random_state=123) # Initialize a decision tree regressor\n\n# Define the parameter grid to tune the hyperparameters\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [2,4,6,8,10,12, 20, 30, None],\n    'min_samples_split': np.arange(1, 10, 1),\n    'min_samples_leaf': np.arange(1, 10, 1),\n    'splitter':[\"best\",\"random\"]\n}\n\nclf_GS = GridSearchCV(rf_pipeline[-1], param_grid)\nclf_GS.fit(X_preproc, y_train)\n\nbest_dtree_reg = clf_GS.best_estimator_ # Get the best estimator from the grid search\n\ny_pred = best_dtree_reg.predict(X_test_preproc)\n\nprint('Best Criterion:', clf_GS.best_estimator_.get_params()['criterion'])\nprint('Best max_depth:', clf_GS.best_estimator_.get_params()['max_depth'])\nprint('Min samples split:', clf_GS.best_estimator_.get_params()['min_samples_split'])\nprint('Min samples leaf:', clf_GS.best_estimator_.get_params()['min_samples_leaf'])\nprint('Best splitter:', clf_GS.best_estimator_.get_params()['splitter'])\n\nprint(f\"\\nBest score:\", clf_GS.best_score_)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Retrain with Hyper-parameters Tuned <a class=\"anchor\" id=\"hp_tuned_retrain\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"tuned_hyper_model = DecisionTreeClassifier( \n                       criterion=str(clf_GS.best_estimator_.get_params()['criterion']), \n                       max_depth=clf_GS.best_estimator_.get_params()['max_depth'],\n                       min_samples_leaf=clf_GS.best_estimator_.get_params()['min_samples_leaf'],\n                       min_samples_split=clf_GS.best_estimator_.get_params()['min_samples_split'],\n                       splitter=str(clf_GS.best_estimator_.get_params()['splitter']),\n                       random_state=123)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fitting model\ntuned_hyper_model.fit(X_preproc, y_train)\n\n# prediction \ntuned_pred=tuned_hyper_model.predict(X_test_preproc)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Metrics <a class=\"anchor\" id=\"hp_tuned_metrics\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"#### Comfusion Matrix <a class=\"anchor\" id=\"hp_confusion_matrix\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"# Plot Confusion Matrix after model hyper parameters tuning\nConfusionMatrixDisplay(confusion_matrix(y_test, tuned_pred)).plot()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest <a class=\"anchor\" id=\"random_forest\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"# Build the Random Forest model\nrf = RandomForestClassifier(random_state=117)\n\nrf_train_start_time = time.time()\n\nrf.fit(X_preproc, y_train)\n\nrf_train_end_time = time.time()\n\nrf_train_time = round(rf_train_end_time - rf_train_start_time, 4)\n\nprint(\"Training time:\", rf_train_time, \"secs\")\n\nrf_train_acc = rf.score(X_preproc, y_train)\nrf_test_acc = rf.score(X_test_preproc, y_test)\n\nprint(f\"Train accuracy: {rf_train_acc:.3f}\")\nprint(f\"Test accuracy: {rf_test_acc:.3f}\")","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metrics <a class=\"anchor\" id=\"rf_metrics\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"#### Classification Report <a class=\"anchor\" id=\"rf_classification_report\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"rf_test_start_time = time.time()\n\nyhat = rf.predict(X_test_preproc)\n\nrf_test_end_time = time.time()\n\nrf_test_time = round(rf_test_end_time - rf_test_start_time, 4)\n\nprint(\"Testing time:\", rf_test_time, \"secs\\n\\n\")\n\nprint(classification_report(y_test, yhat))","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Confusion Matrix <a class=\"anchor\" id=\"rf_confusion_matrix\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, yhat)\ncm_display = ConfusionMatrixDisplay(cm).plot()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance <a class=\"anchor\" id=\"rf_feature_imp\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"# feature importances\nimportances = pd.Series(\n    rf.feature_importances_, index=X_preproc.columns\n).sort_values(ascending=True).plot.barh()\nplt.title('Feature Importances (Random Forest)')\nplt.show()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Boosting <a class=\"anchor\" id=\"gradiet_boosting\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"# Build the Gradient Boosting model\ngb = GradientBoostingClassifier(random_state=117)\n\ngb_train_start_time = time.time()\n\ngb.fit(X_preproc, y_train)\n\ngb_train_end_time = time.time()\n\ngb_train_time = round(gb_train_end_time-gb_train_start_time, 4)\n\nprint(\"Training time:\", gb_train_time, \"secs\")\n\ngb_train_acc = gb.score(X_preproc, y_train)\ngb_test_acc = gb.score(X_test_preproc, y_test)\n\nprint(f\"Train accuracy: {gb_train_acc:.3f}\")\nprint(f\"Test accuracy: {gb_test_acc:.3f}\")","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metrics <a class=\"anchor\" id=\"gb_metrics\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"### Classification Report <a class=\"anchor\" id=\"gb_classification_report\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"gb_test_start_time = time.time()\n\nyhat = gb.predict(X_test_preproc)\n\ngb_test_end_time = time.time()\n\ngb_test_time = round(gb_test_end_time - gb_test_start_time, 4)\n\nprint(\"Testing time:\", gb_test_time, \"secs\\n\\n\")\n\nprint(classification_report(y_test, yhat))","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion Matrix <a class=\"anchor\" id=\"gb_confusion_matrix\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, yhat)\ncm_display = ConfusionMatrixDisplay(cm).plot()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance <a class=\"anchor\" id=\"gb_feature_importance\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"# feature importances\nimportances = pd.Series(\n    gb.feature_importances_, index=X_preproc.columns\n).sort_values(ascending=True).plot.barh()\nplt.title('Feature Importances (Gradient Boosting)')\nplt.show()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AdaBoost <a class=\"anchor\" id=\"AdaBoost\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"# Build the AdaBoost model\nada = AdaBoostClassifier(algorithm=\"SAMME\", random_state=117)\n\nab_train_start_time = time.time()\n\nada.fit(X_preproc, y_train)\n\nab_train_end_time = time.time()\n\nada_train_time = round(ab_train_end_time - ab_train_start_time, 4)\nprint(\"Training time:\", ada_train_time, \"secs\")\n\nada_train_acc = ada.score(X_preproc, y_train)\nada_test_acc = ada.score(X_test_preproc, y_test)\n\nprint(f\"Train accuracy: {ada_train_acc:.3f}\")\nprint(f\"Test accuracy: {ada_test_acc:.3f}\")","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metrics <a class=\"anchor\" id=\"AB_metrics\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"### Classification Report <a class=\"anchor\" id=\"AB_classification_report\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"ab_test_start_time = time.time()\n\nyhat = ada.predict(X_test_preproc)\n\nab_test_end_time = time.time()\n\nada_test_time = round(ab_test_end_time - ab_test_start_time, 4)\n\nprint(\"Testing time:\", ada_test_time, \"secs\\n\\n\")\n\nprint(classification_report(y_test, yhat))","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion Matrix <a class=\"anchor\" id=\"AB_confusion_matrix\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, yhat)\ncm_display = ConfusionMatrixDisplay(cm).plot()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance <a class=\"anchor\" id=\"AB_feature_importance\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{"tags":[]}},{"cell_type":"code","source":"# feature importances\nimportances = pd.Series(\n    ada.feature_importances_, index=X_preproc.columns\n).sort_values(ascending=True).plot.barh()\nplt.title('Feature Importances (AdaBoost)')\nplt.show()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models Comparison <a class=\"anchor\" id=\"models_comparison\"></a> <p style=\"text-align: right; color: blue; font-size: 15px\"> [Go to Main Menu](#toc) </p>","metadata":{}},{"cell_type":"code","source":"bar_colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:orange', '#A890F0']\n\nmodels = ['Decision Tree without pruning',\n          'Decision Tree with pruning', \n          'Random Forest', \n          'Gradient Boosted', \n          'Adaboost']\n\ntraining_time = [dt_train_time, dtp_train_time, \n                 rf_train_time, gb_train_time, ada_train_time]\n\ntesting_time = [dt_test_time, dtp_test_time, rf_test_time, \n                gb_test_time, ada_test_time]\ntrain_accuracy = [dt_train_acc, dtp_train_acc, rf_train_acc, \n                  gb_train_acc, ada_train_acc]\ntest_accuracy = [dt_test_acc, dtp_test_acc, rf_test_acc, \n                 gb_test_acc, ada_test_acc]\n\n#fig, ax = plt.subplots()\nfig, ax = plt.subplots(4, figsize=(10,20))\n\n#plt.figure(figsize = (25, 10))\n#plt.subplot(2, 2, 1)\n# creating the training bar plot\nax[0].bar(models, training_time, \n        color = bar_colors, \n        label=models,\n        width = 0.4)\n\n#ax.set_xlabel(\"Model\")\n#ax.xticks(rotation=90)\nax[0].set_xticks([])\nax[0].set_ylabel(\"Training Time\")\nax[0].set_title(\"Training time comparison of models\")\nax[0].legend(title = \"Models\", loc='center left', bbox_to_anchor=(1, 0.5))\n\n# plt.subplot(2, 2, 2)\n# # creating the Testing bar plot\nax[1].bar(models, testing_time, color = bar_colors, \n        label=models, \n        width = 0.4)\n\nax[1].set_xticks([])\nax[1].set_ylabel(\"Testing Time\")\nax[1].set_title(\"Testing time comparison of models\")\nax[1].legend(title = \"Models\", loc='center left', bbox_to_anchor=(1, 0.5))\n\n# plt.subplot(2, 2, 3)\n# # creating the Testing bar plot\nax[2].bar(models, train_accuracy, color = bar_colors, \n        label=models,\n         width = 0.4)\n\n# plt.xlabel(\"Model\")\n# plt.xticks(rotation=90)\nax[2].set_xticks([])\nax[2].set_ylabel(\"Training Accuracy\")\nax[2].set_ylim(0.7, 1)\nax[2].set_title(\"Training Accuracy comparison of models\")\nax[2].legend(title = \"Models\", loc='center left', bbox_to_anchor=(1, 0.5))\n          \n# plt.subplot(2, 2, 4)\n# # creating the Testing bar plot\nax[3].bar(models, test_accuracy, color = bar_colors, \n        label=models,width = 0.4)\n\n# plt.xlabel(\"Model\")\n# plt.xticks(rotation=90)\nax[3].set_xticks([])\nax[3].set_ylabel(\"Test Accuracy\")\nax[3].set_ylim(0.7, 0.9)\nax[3].set_title(\"Test Accuracy comparison of models\")\nax[3].legend(title = \"Models\", loc='center left', bbox_to_anchor=(1, 0.5))\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summary\n- Random forest shows the highest accuracy but with less testing accuracy, an indication of overfit.\n- Adaboost metrics shows the balanced training and testing accuracy with least execution(training and testing) time.\n- Gradient boost has the highest testing accuracy.\n\n## Future Work:\n- The Hyper parameters can be tuned for random forest, Adaboost and Gradient boost algorithms.\n- Further Feature engineering can be performed to enhance the performance of the algorithms.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
